{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "![](task1a_1.png)\n",
    "![](task1a_2.png)\n",
    "![](task1a_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "Using np.mean() and np.std() on the whole training set, we get <br>\n",
    "$ \\mu = 33.55274553571429$ and <br>\n",
    "$ \\sigma = 78.87550070784701$ <br>\n",
    "We store these as default values in the preprocessing function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss_final.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just use the weight matrices that we have. The first is 785 x 64 and the second is 64 x 10. In total 50 880 parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice an increase in learning speed and accuracy when implementing the 3 tricks. The above figure shows the difference when implementing the three tricks (a contains use_improved_weight_unit, b containains same as a + use_imprvoed_sigmoid and c contains the same tricks as b + use_momentum).\n",
    "<br>\n",
    "Below we show the same plot but with training accuracy as well to look for overfitting.\n",
    "<br>\n",
    "<br>\n",
    "The biggest jump in performance comes from implementing the first step. Wether this is because initializing the weight using the given distribution is particularly useful, or whether it is because the network had more potenital for improvement before any of the tricks are implemented, is unclear and has to be tested by implementing the tricks in different orders.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "In total, the convergence rate (using early stop) is doubled. The number of epochs before stopping without any tricks was 33, while the number of epochs with all tricks is 15. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "We can see from the below plot that the training accuracy also increases drastically for each trick (the final train accuracy was .99995), but wether it increases more than the validation accuracy is unclear. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "We can also see that implementing the momentum actually had a slightly negative impact, compared to only using improved sigmoid and improved weight initialization. However, there might be other values of momentum_gamma and learning speed that we could use to change this fact"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](task3_with_training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "![](task4a.png)\n",
    "\n",
    "We can see that both training av avlidation accuracy is signficantly lower when using only 32 units in the hidden layer. The convergence is about the same (both 32-units and 64-units require ca. 16000 training steps to not see validation improvement over 50 steps).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "![](task4b.png)\n",
    "We can see that especially the validation accuracy is significantly lower, compared to the using 64 units in the hidden layer. It converges very fast, and this could indicate that the model is overfitted. This is supported by the fact that there is near perfect training accuracy while the validation accuracy stagnates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "In the previous network, we had 785x64 + 64x10 = 50880 paramters. This is with inputs [64, 10] to the NN (1 hidden layer of size 64). <br>\n",
    "In this task, we will usa a 785x60 + 60x60 + 60x10 = 51300 paramters. This is with inputs [60, 60, 10] to the NN (2 hidden layers of size 60).\n",
    "\n",
    "![](task4d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "af5f898791abf7f5ddcab079bab48338502e0e6bdef72dc5f0a1b8add6b0c950"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
